{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "w11_Quiz_Linear_models",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/RayleighKim/Coursera_Advanced_Machine_Learning_Specialization/blob/master/01_Introduction_To_DeepLearning/w11_Quiz_Linear_models.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "M5urgouueCxS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Quiz : Linear Models\n",
        "---\n",
        "Rayleigh Kim @ dplus\n",
        "\n",
        "email1 : rayleigh@dplus.company<br>\n",
        "email2 : wood.rayleigh@gmail.com\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "GdSYY0wXfDYc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Q1. Consider a vector (1, -2, 0.5). Apply a softmax transform to it and enter the first component"
      ]
    },
    {
      "metadata": {
        "id": "qIfQ2NGue8ns",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "outputId": "c4c093da-37f6-443f-a641-a2e0d8d18e12"
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "vec = np.array([1,-2,0.5]) # make a vector (1, -2, 0.5)\n",
        "\n",
        "numerator = np.exp(vec) # Numerator : make a exponent vector (exp(1), exp(-2), exp(0.5))\n",
        "\n",
        "denominator = np.sum(vec_exp) # Denominator\n",
        "\n",
        "softmax_vec = np.divide(numerator, denominator)\n",
        "\n",
        "print(numerator, denominator, softmax_vec, sep='\\n')\n",
        "print('\\n','The Answer is {:.3f}'.format(softmax_vec[0]))\n",
        "\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[2.71828183 0.13533528 1.64872127]\n",
            "4.502338382395786\n",
            "[0.6037489  0.03005889 0.36619222]\n",
            "\n",
            " The Answer is 0.604\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "NNzc6I1mg2eg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Q2. Suppose you are solving a 5-class classification problem with 10 features. How many parameters a linear modle would have? Don't forget bias terms!"
      ]
    },
    {
      "metadata": {
        "id": "mr6QxtRhhVzt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "fc8a444d-4613-42c8-f078-910d7ec6a706"
      },
      "cell_type": "code",
      "source": [
        "params_for_one_class = 10 +1 # 10 parameters for 10 Features, 1 parameter for bias.\n",
        "\n",
        "answer = 5*params_for_one_class # 5-class classification is do [classification one vs all] five times\n",
        "\n",
        "print('The Answer is {}'.format(answer))\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The Answer is 55\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "mldNyHxdfisT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Q3. There is an analytical solution for linear regression parameters and MSE loss, but we usually prefer gradient descent optimization over it. what are the reasons?\n",
        "\n",
        "* Gradient descent is a method developed especially for MSE loss\n",
        "* Gradient descent doesn't require to invert a matrix\n",
        "* Gradient descent can find parameter values that give lower MSE value than parameters from analytical solution\n",
        "* Gradeint descent is more scalable and can be applied for problems with high number of features\n",
        "\n",
        "\n",
        "Answer : Yes I don't need to code!\n",
        "\n",
        "\n",
        "* Gradient descent is a method developed **not** especially for MSE loss\n",
        "* Gradient descent doesn't require to invert a matrix\n",
        "* Gradient descent can find parameter values that give **higher (or equal to)**~~lower~~ MSE value than parameters from analytical solution\n",
        "* Gradeint descent is more scalable and can be applied for problems with high number of features"
      ]
    },
    {
      "metadata": {
        "id": "CsW6lqlcffbJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}