{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "w53_Quiz_Modern_RNNs.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/RayleighKim/Coursera_Advanced_Machine_Learning_Specialization/blob/master/01_Introduction_To_DeepLearning/w53_Quiz_Modern_RNNs.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "M5urgouueCxS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Quiz : Modern_RNNs\n",
        "---\n",
        "Rayleigh Kim @ dplus\n",
        "\n",
        "email1 : rayleigh@dplus.company<br>\n",
        "email2 : wood.rayleigh@gmail.com\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "GdSYY0wXfDYc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Q1. Choose correct statements about the exploding gradient problem:\n",
        "\n",
        "* Exploding gradient problem is easy to detect : **True, You can see the loss**\n",
        "* ReLU nonlinearity helps with the exploding gradient problem. : **False, It helps with vanishing gradient problem**\n",
        "* The reason of the exploding gradient problem in the simple RNN is the recurrent weight matrix $W$. Nonlinearities sigmoid, tanh, and ReLU does not cause the problem. **True $W$ cause the vanishing or exploding gradient, and large $W$ cause exploding gradients, especially with ReLU**\n",
        "* The threshold for gradient clipping should be as low as possible to make the training more efficient **False, as high as**"
      ]
    },
    {
      "metadata": {
        "id": "NNzc6I1mg2eg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Q2.Choose correct statements about the vanishing gradient problem :\n",
        "\n",
        "* Vanishing gradient problem is easy to detect. **False, It's hard to detect. convergence or vanishing gradient**\n",
        "* Both nonlinearity and the recurrent weight matrix $W$ cause the vanishing gradient problem. **True.**\n",
        "* Orthogonal Initialization of the recurrent weight matrix helps with vanishing gradient problem **True**\n",
        "* Truncated BPTT helps with the vanishing gradient problem. **False. it makes the gradient from faraway steps equal to zero. It's for exploding gradients**"
      ]
    },
    {
      "metadata": {
        "id": "mldNyHxdfisT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Q3. Consider the LSTM architecture:\n",
        "\n",
        "$ \\begin{pmatrix} g_t \\\\ i_t \\\\ o_t\\\\f_t \\end{pmatrix} = \\begin{pmatrix} \\tilde f \\\\ \\sigma \\\\ \\sigma \\\\ \\sigma  \\end{pmatrix}(Vx_t + wh_{t-1}+b) $\n",
        "\n",
        "$c_t = f_t \\cdot c_{t-1} +i_t\\cdot g_t $<br> $ h_t =o_t\\cdot\\tilde f (c_t)$\n",
        "\n",
        "![image from coursera](https://d3c33hcgiwev3.cloudfront.net/imageAssetProxy.v1/rgAJk58fEee49A5MAYfNMg_12d6c5d789776163bd9b7493fbd5b02f_Screen-Shot-2017-09-22-at-01.49.46.png?expiry=1533427200000&hmac=xZ2T9E2lPLL5XgOSMVUHv45-pxDinfyR1sL1kxZ1K7s)\n",
        "\n",
        "Choose correct statements about this architecture:\n",
        "\n",
        "* The LSTM needs four times more parameters than the simple RNN. **True, $V$ is not just single $V$. it consists of $V_f, V_i, V_g, V_o$ **\n",
        "* Gradients do not vanish on the way through memory cells $c$ in the LSTM with forget gate. **False  ${\\partial c_t \\over \\partial c_{t-1}} = f_t$** and forget gate may take values from 0 to 1, therefore vanishing gradient problem is still possible here and careful initialization of forget gate is needed\n",
        "* There is a combination of the gates values which makes the LSTM completely equivalent to the simple RNN **False, The LSTM is very similar to the simple RNN when input and output gates are open and forget gate is closed. However, they are not completely equivalent because in the LSTM nonlinearity $\\tilde f$ is used twice on the way between $h_{t-1}$ to $h_t$**\n",
        "* The exploding gradient problem is still possible in the LSTM on the way between $h_{t-1}$ and $h_t$ **True, Very large norm of $W$ may cause the exploding gradient problem. Therefore gradient clipping is usefull for LSTM and GRU architectures too**"
      ]
    },
    {
      "metadata": {
        "id": "CsW6lqlcffbJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Q4. Consider the GRU architecture:\n",
        "\n",
        "\n",
        "$g_t = \\tilde f(V_g x_t + W_g(h_{t-1}\\cdot r_t)+b_g )$<br>\n",
        "$ h_t =(1-u_t)\\cdot g_t +u_t \\cdot h_{t-1} $\n",
        "\n",
        "![image from coursera](https://d3c33hcgiwev3.cloudfront.net/imageAssetProxy.v1/71G3Bp8cEee49A5MAYfNMg_0a7ca641867c6bdefc3084439990dce4_Screen-Shot-2017-09-22-at-01.30.05.png?expiry=1533427200000&hmac=K0BKKSS6WaB2ysWU0d--uvhKmoXGYMhwhrrBUvmX16Q)\n",
        "\n",
        "Which combination of the gate values makes this model equivalent to the simple RNN? Here value zero corresponds to a closed gate and value one corresponds to an open gate.\n",
        "\n",
        "\n",
        "* Reset gate is open and update gate is closed"
      ]
    },
    {
      "metadata": {
        "id": "KlU7djgsymxj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}