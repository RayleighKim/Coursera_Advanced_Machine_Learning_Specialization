{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "w42_Quiz_Word_embeddings",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/RayleighKim/Coursera_Advanced_Machine_Learning_Specialization/blob/master/01_Introduction_To_DeepLearning/w42_Quiz_Word_embeddings.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "M5urgouueCxS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Quiz : Word embeddings\n",
        "---\n",
        "Rayleigh Kim @ dplus\n",
        "\n",
        "email1 : rayleigh@dplus.company<br>\n",
        "email2 : wood.rayleigh@gmail.com\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "GdSYY0wXfDYc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Q1. Which of the following is true about word2vec model? :\n",
        "\n",
        "* It's outputs (predictions) are linear functions of inputs : **False, It is not the one of GLMs**\n",
        "* It requires some text corpora for training : **True**\n",
        "* It uses convolutional layers and pooling : **False, It is not CNN**\n",
        "* It has one trainable parameter per word : **False**\n",
        "* It requires human-defined semantic relations between words **False, doesn't require human-defined semantic relations**"
      ]
    },
    {
      "metadata": {
        "id": "NNzc6I1mg2eg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Q2. How can you train word2vec model?:\n",
        "* By Learning to predict omitted word by it's context **True, CBOW**\n",
        "* By applying stochastic gradient descent **True, It's good for you!**\n",
        "* By minimizing distance between human-defined synonyms and maximizing distance between antonyms **False, not human-defined..**\n",
        "* By minimizing crossentropy (a.k.a maximizing likelihood)) **True**\n",
        "* By learning to predict context (neighboring words) given one word. **True, Skip-gram**\n",
        "* By changing order or words in the corpora. **False**"
      ]
    },
    {
      "metadata": {
        "id": "mldNyHxdfisT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Q3. Here's an [online demo](http://bionlp-www.utu.fi/wv_demo/) of word2vec model. Let's use it to find synonyms for rare words. Don't forget to choose English GoogleNes model. Which of the following words is in top 10 synonyms for 'weltschmerz'\n",
        "\n",
        "* Let's Do it!"
      ]
    },
    {
      "metadata": {
        "id": "CsW6lqlcffbJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Q4. Which of the following is an appropriate way to measure similarity between word vectors v1 and v2? (more = better)\n",
        "\n",
        "* $$-||v1 - v2|| $$\n",
        "* $$||v1 - v2||$$\n",
        "* $$ sin(v1, v2) $$\n",
        "* $$ cos(v1, v2) $$\n",
        "\n",
        "cos similarity...and -1*L1 distance\n",
        "\n",
        "The more similar Two word vectors, the bigger. "
      ]
    },
    {
      "metadata": {
        "id": "KlU7djgsymxj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}